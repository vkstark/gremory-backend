# Gremory Backend (`gremory-be`)

**Backend for [Gremory](https://github.com/vkstark/gremory-app)** â€“ your private AI assistant. Fully customizable, self-hostable, and model-agnostic.

Gremory enables you to chat using your own models, APIs, and infrastructure. Unlike hosted LLMs, your data stays where **you** choose â€“ in your cloud, database, or offline.

<br>

## ğŸ”— Related Projects

* ğŸ”µ **Frontend (Flutter):** [`vkstark/gremory-app`](https://github.com/vkstark/gremory-app)
* ğŸŒ Works across Android, iOS, macOS, Web

<br>

## âš™ï¸ Tech Stack

| Layer                   | Tech                                                       |
| ----------------------- | ---------------------------------------------------------- |
| **Database**            | PostgreSQL (hosted on AWS)                                 |
| **Backend**             | Python + FastAPI                                           |
| **Model Orchestration** | LangChain (used for LLM abstraction & chaining)            |
| **Frontend**            | Flutter (cross-platform)                                   |
| **Deployment**          | Docker                                                     |
| **Infra Notes**         | AWS Amplify tested, but currently self-managed due to cost |

> The system is containerized and easily portable to any cloud or local stack.

<br>

## ğŸ§  Features

* ğŸ”Œ **Bring your own models** â€“ OpenAI, local, or custom inference engines
* ğŸ§¾ **Chat + User History APIs**
* ğŸš€ **Deploy anywhere** with Docker or cloud platforms
* ğŸ§ª **Pytest-based testing**
* ğŸ§° Modular structure inspired by best open-source LLM backends

<br>

## âš¡ï¸ Quickstart

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Setup Environment

```bash
cp .env.example .env
```

Edit it with your values (OpenAI key, DB credentials, etc.)

### 3. Run Server

```bash
uvicorn main:app --reload
```

Swagger UI: `http://localhost:8000/docs`

<br>

## ğŸ§© Add Your Own Model

To integrate a custom model:

1. **Edit `_get_or_create_model` in** `services/chat_service.py`
2. Register the model in `self.model_mapping`
3. Add the model to the `SupportedModels` Enum in `app/api/chat.py`
4. Add any required config/keys to `.env`

> This lets you plug in local LLMs, third-party APIs, or anything else.

<br>

## ğŸŒ API Overview

### ğŸ”¹ Chat & Models

* `GET /api/v1/models`
* `POST /api/v1/chat`

### ğŸ”¸ Conversations & History

* Full CRUD + messaging on user history and conversations

### ğŸ‘¤ Users

* User management and test seeding

<br>

## ğŸ§ª Testing

```bash
pytest
```

Covers chat, history, and bug regressions.

<br>

## ğŸš€ Deployment

* âœ… Dockerized and ready for Render, Railway, Fly.io, AWS, etc.
* ğŸŒ Easily runs on VPS or on-premises
* ğŸ›  Auto-configured via `.env`


<br>

## ğŸ™Œ Contribute

Pull requests welcome!
Ideas, tools, or your own model integrations? Start a discussion or fork and build.


<br>

## ğŸ›  To-Do / Roadmap

* ğŸ”„ **Personalization** using user-specific data
* ğŸ“„ **RAG (Retrieval-Augmented Generation)** with document uploads
* ğŸ”§ **Tool calling / actions** like calculators, web search, etc.
* ğŸ¤– **Multi-agent system** with routing and dynamic task delegation
